{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "        \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        self.W = [np.random.randn(input_dim, hidden_dim)*((2/(input_dim+hidden_dim))**0.5)]\n",
    "        self.b = [np.zeros(hidden_dim)]\n",
    "        self.gradW = [np.zeros((input_dim, hidden_dim))]\n",
    "        self.gradb = [np.zeros(hidden_dim)]\n",
    "        for i in range(num_layers):\n",
    "            self.W.append(np.random.randn(hidden_dim, hidden_dim)*((1/hidden_dim)**0.5))\n",
    "            self.b.append(np.zeros(hidden_dim))\n",
    "            self.gradW.append(np.zeros((hidden_dim, hidden_dim)))\n",
    "            self.gradb.append(np.zeros(hidden_dim))\n",
    "        self.W.append(np.random.randn(hidden_dim, output_dim)*((2/(hidden_dim+output_dim))**0.5))\n",
    "        self.b.append(np.zeros(output_dim))\n",
    "        self.gradW.append(np.zeros((hidden_dim, output_dim)))\n",
    "        self.gradb.append(np.zeros(output_dim))\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = np.array(self.W[i] , dtype = np.longdouble)\n",
    "        for i in range(len(self.b)):\n",
    "            self.b[i] = np.array(self.b[i] , dtype= np.longdouble )\n",
    "        self.activations = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations = [x.copy()]\n",
    "        for i in range(len(self.W)-1):\n",
    "            x = np.maximum(x @ self.W[i] + self.b[i], 0)\n",
    "            self.activations.append(x.copy())\n",
    "        x = x @ self.W[-1] + self.b[-1]\n",
    "        return x\n",
    "\n",
    "    def backward(self, error):\n",
    "        self.gradW[-1] = self.activations[-1].reshape(-1, 1) @ error.reshape(1, -1)\n",
    "        self.gradb[-1] = error.copy().T\n",
    "        for i in reversed(range(len(self.W)-1)):\n",
    "            error = (error @ self.W[i+1].T)  * (self.activations[i+1]>0).astype(int)\n",
    "            self.gradW[i] = self.activations[i].reshape(-1, 1) @ error.reshape(1, -1)\n",
    "            self.gradb[i] = error.copy().T\n",
    "        self.gradW = [ np.clip(w,  1,  -1)for w in self.gradW]\n",
    "        self.gradb = [ np.clip(b, 1,  -1)for b in self.gradb]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = self.W[i] - lr * self.gradW[i]\n",
    "            self.b[i] = self.b[i] - lr * self.gradb[i]\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(\" + \", \".join([str(x.shape[0]) for x in self.W]) + \", \" + str(self.W[-1].shape[1]) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\batu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\batu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04297874,  0.04063813, -0.00089307,  0.04530634], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetNet(net):\n",
    "    netCopy = Network(1,1,1,1)\n",
    "    netCopy.W = copy.deepcopy(net.W)\n",
    "    netCopy.b = copy.deepcopy(net.b)\n",
    "    netCopy.gradb = copy.deepcopy(net.gradb)\n",
    "    netCopy.gradW = copy.deepcopy(net.gradW)\n",
    "    netCopy.activations = net.activations\n",
    "    return netCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(x):\n",
    "    r = random.random()\n",
    "    global epsilon\n",
    "    if r < epsilon:\n",
    "        return random.choice([0,1])\n",
    "    else:\n",
    "        return np.argmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(4, 16, 2, 1) # (MLP structure: 4 -- 16 -- 16 -- 2)\n",
    "targetNet = getTargetNet(net)\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99 # for the horizon\n",
    "epsilon = 1.0 # decay it with 0.999 after each episode and fix it at 0.1\n",
    "epsilonDecay = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [132], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m         target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m  gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(targetNet\u001b[38;5;241m.\u001b[39mforward(observation))\n\u001b[0;32m     26\u001b[0m     error \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m (target \u001b[38;5;241m-\u001b[39m x[action]) \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m i \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     net\u001b[38;5;241m.\u001b[39mupdate(learning_rate)\n\u001b[0;32m     29\u001b[0m targetNet \u001b[38;5;241m=\u001b[39m getTargetNet(net)\n",
      "Cell \u001b[1;32mIn [37], line 37\u001b[0m, in \u001b[0;36mNetwork.backward\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m     35\u001b[0m     error \u001b[38;5;241m=\u001b[39m (error \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT)  \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m error\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradb[i] \u001b[38;5;241m=\u001b[39m \u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW \u001b[38;5;241m=\u001b[39m [ np\u001b[38;5;241m.\u001b[39mclip(w,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradb \u001b[38;5;241m=\u001b[39m [ np\u001b[38;5;241m.\u001b[39mclip(b, \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradb]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = 1.0\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    replayBuffer = []\n",
    "    for i in range(1000):\n",
    "        \n",
    "        x = net.forward(state)\n",
    "        action = policy(x)\n",
    "        replayBuffer.append((state,x, action, env.step(action)))\n",
    "        observation, reward, terminated, info = replayBuffer[-1][-1]\n",
    "        if terminated:\n",
    "            env.reset()\n",
    "    errors = []\n",
    "    for frame in np.random.choice(range(len(replayBuffer)), 320, replace=False) :\n",
    "        frame = replayBuffer[frame]\n",
    "        state,x , action, step = frame\n",
    "        observation, reward, terminated, info = step\n",
    "        # x = targetNet.forward(state)\n",
    "        # net.gradW = targetNet.gradW\n",
    "        # net.gradb = targetNet.gradb\n",
    "        x = net.forward(state)\n",
    "        if terminated:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward +  gamma * np.max(targetNet.forward(observation))\n",
    "        error = [-1 * (target - x[action]) if action == i else 0 for i in [0,1]]\n",
    "        net.backward(np.array(error).reshape(-1))\n",
    "        net.update(learning_rate)\n",
    "    targetNet = getTargetNet(net)\n",
    "    epsilon *= epsilonDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimatePerformance():\n",
    "    global epsilon    \n",
    "    counters = []\n",
    "    exp = epsilon\n",
    "    epsilon = -1\n",
    "    for i in range(100):\n",
    "        counter = 0\n",
    "        terminated = False\n",
    "        observation = env.reset()\n",
    "        while not terminated:\n",
    "            action = policy(net.forward(observation))\n",
    "            observation, reward, terminated, info = env.step(action)\n",
    "            counter += 1\n",
    "        counters.append(counter)\n",
    "\n",
    "    summ =  0\n",
    "    for i in range(100):\n",
    "        summ += counters[i]\n",
    "    print(summ/100)\n",
    "    epsilon = exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06445999e+12, 2.06454362e+12])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.28\n"
     ]
    }
   ],
   "source": [
    "estimatePerformance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1.1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1\n",
    "x = [0.1,1.1]\n",
    "np.array([x[i] if action == i else 0 for i in [0,1]]).reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3788125040940492e-05"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [144], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m         error \u001b[38;5;241m=\u001b[39m  (reward \u001b[38;5;241m-\u001b[39m x[action]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m         error \u001b[38;5;241m=\u001b[39m [error \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m i \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m---> 26\u001b[0m         \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         net\u001b[38;5;241m.\u001b[39mupdate(learning_rate)\n\u001b[0;32m     29\u001b[0m targetNet \u001b[38;5;241m=\u001b[39m getTargetNet(net)\n",
      "Cell \u001b[1;32mIn [37], line 36\u001b[0m, in \u001b[0;36mNetwork.backward\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     35\u001b[0m     error \u001b[38;5;241m=\u001b[39m (error \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT)  \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradb[i] \u001b[38;5;241m=\u001b[39m error\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW \u001b[38;5;241m=\u001b[39m [ np\u001b[38;5;241m.\u001b[39mclip(w,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replayBuffer = []\n",
    "net = Network(4, 16, 2, 1)\n",
    "targetNet = getTargetNet(net)\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    for i in range(1000):\n",
    "        x = net.forward(state)\n",
    "        action = policy(x)\n",
    "\n",
    "        replayBuffer.append((state,x, action, env.step(action)))\n",
    "        observation, reward, terminated, info = replayBuffer[-1][-1]\n",
    "        if terminated:\n",
    "            env.reset()\n",
    "        if(len(replayBuffer) < 32):\n",
    "            continue\n",
    "        for frame in np.random.choice(range(len(replayBuffer)), 32, replace=False) :\n",
    "            frame = replayBuffer[frame]\n",
    "            state,x , action, step = frame\n",
    "            observation, reward, terminated, info = step\n",
    "            x = net.forward(state)\n",
    "            if not terminated: \n",
    "                reward += gamma * np.max(targetNet.forward(observation))\n",
    "            error =  (reward - x[action]) * -1\n",
    "            error = [error if action == i else 0 for i in [0,1]]\n",
    "            \n",
    "            net.backward(np.array(error))\n",
    "            net.update(learning_rate)\n",
    "    \n",
    "    targetNet = getTargetNet(net)\n",
    "    if epsilon > 10 ** -1:\n",
    "        epsilon *= epsilonDecay\n",
    "    replayBuffer = []\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a59fb673d176b02a6017ed87a6c53d78aed30df45046877791ad08793a0c669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
