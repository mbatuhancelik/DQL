{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "\n",
    "        self.W = [np.random.randn(input_dim, hidden_dim)*((2/(input_dim+hidden_dim))**0.5)]\n",
    "\n",
    "        self.b = [np.zeros(hidden_dim)]\n",
    "\n",
    "        self.gradW = [np.zeros((input_dim, hidden_dim))]\n",
    "\n",
    "        self.gradb = [np.zeros(hidden_dim)]\n",
    "\n",
    "        for i in range(num_layers):\n",
    "\n",
    "            self.W.append(np.random.randn(hidden_dim, hidden_dim)*((1/hidden_dim)**0.5))\n",
    "\n",
    "            self.b.append(np.zeros(hidden_dim))\n",
    "\n",
    "            self.gradW.append(np.zeros((hidden_dim, hidden_dim)))\n",
    "\n",
    "            self.gradb.append(np.zeros(hidden_dim))\n",
    "\n",
    "        self.W.append(np.random.randn(hidden_dim, output_dim)*((2/(hidden_dim+output_dim))**0.5))\n",
    "\n",
    "        self.b.append(np.zeros(output_dim))\n",
    "\n",
    "        self.gradW.append(np.zeros((hidden_dim, output_dim)))\n",
    "\n",
    "        self.gradb.append(np.zeros(output_dim))\n",
    "\n",
    "        self.activations = []\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.activations = [x.copy()]\n",
    "\n",
    "        for i in range(len(self.W)-1):\n",
    "\n",
    "            x = np.maximum(x @ self.W[i] + self.b[i], 0)\n",
    "\n",
    "            self.activations.append(x.copy())\n",
    "\n",
    "        x = x @ self.W[-1] + self.b[-1]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, error):\n",
    "\n",
    "        self.gradW[-1] = self.activations[-1].reshape(-1, 1) @ error.reshape(1, -1)\n",
    "\n",
    "        self.gradb[-1] = error.copy()\n",
    "\n",
    "        for i in reversed(range(len(self.W)-1)):\n",
    "\n",
    "            error = (error @ self.W[i+1].T) * (self.activations[i+1]>0).astype(int)\n",
    "\n",
    "            self.gradW[i] = self.activations[i].reshape(-1, 1) @ error.reshape(1, -1)\n",
    "\n",
    "            self.gradb[i] = error.copy()\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "\n",
    "        for i in range(len(self.W)):\n",
    "\n",
    "            self.W[i] = self.W[i] - lr * self.gradW[i]\n",
    "\n",
    "            self.b[i] = self.b[i] - lr * self.gradb[i]\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"(\" + \", \".join([str(x.shape[0]) for x in self.W]) + \", \" + str(self.W[-1].shape[1]) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\batu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\batu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02948984, -0.03443295,  0.00815394, -0.01692887], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetNet(net):\n",
    "    netCopy = Network(1,1,1,1)\n",
    "    netCopy.W = copy.deepcopy(net.W)\n",
    "    netCopy.b = copy.deepcopy(net.b)\n",
    "    netCopy.gradb = copy.deepcopy(net.gradb)\n",
    "    netCopy.gradW = copy.deepcopy(net.gradW)\n",
    "    netCopy.activations = copy.deepcopy(net.activations)\n",
    "    return netCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(x, epsilon):\n",
    "    r = random.random()\n",
    "    if r < epsilon:\n",
    "        return random.choice([0,1])\n",
    "    else:\n",
    "        return np.argmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimatePerformance():  \n",
    "    counters = []\n",
    "    epsilon = -1\n",
    "    trials = 10\n",
    "    for i in range(trials):\n",
    "        counter = 0\n",
    "        terminated = False\n",
    "        observation = env.reset()\n",
    "        while not terminated:\n",
    "            action = np.argmax(net.forward(observation))\n",
    "            observation, reward, terminated, info = env.step(action)\n",
    "            counter += reward\n",
    "        counters.append(counter)\n",
    "\n",
    "    summ =  0\n",
    "    for i in range(trials):\n",
    "        summ += counters[i]\n",
    "    return summ/trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(4, 16, 2, 1) # (MLP structure: 4 -- 16 -- 16 -- 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetNet = getTargetNet(net)\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99 # for the horizon\n",
    "epsilon = 1.0 # decay it with 0.999 after each episode and fix it at 0.1\n",
    "epsilonDecay = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 23.5\n",
      "2000 112.2\n",
      "3000 276.4\n",
      "4000 271.1\n",
      "5000 187.0\n",
      "6000 361.5\n",
      "7000 500.0\n",
      "8000 500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [74], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(observation)\n\u001b[1;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     replayBuffer\u001b[38;5;241m.\u001b[39mappend((observation,x, action, env\u001b[38;5;241m.\u001b[39mstep(action)))\n\u001b[0;32m     13\u001b[0m     observation, reward, terminated, info \u001b[38;5;241m=\u001b[39m replayBuffer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn [5], line 6\u001b[0m, in \u001b[0;36mpolicy\u001b[1;34m(x, epsilon)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = 1.0\n",
    "counter = 0\n",
    "episode = 0\n",
    "while True:\n",
    "    observation = env.reset()\n",
    "    replayBuffer = []\n",
    "    targetNet = getTargetNet(net)\n",
    "    for i in range(1000):\n",
    "        \n",
    "        x = net.forward(observation)\n",
    "        action = policy(x, epsilon)\n",
    "        replayBuffer.append((observation,x, action, env.step(action)))\n",
    "        observation, reward, terminated, info = replayBuffer[-1][-1]\n",
    "        if terminated:\n",
    "            observation = env.reset()\n",
    "    errors = []\n",
    "    for frame in np.random.choice(range(len(replayBuffer)), 32, replace=False) :\n",
    "        frame = replayBuffer[frame]\n",
    "        state,x , action, step = frame\n",
    "        observation, reward, terminated, info = step\n",
    "        # x = targetNet.forward(state)\n",
    "        # net.gradW = targetNet.gradW\n",
    "        # net.gradb = targetNet.gradb\n",
    "        x = net.forward(state)\n",
    "        if terminated:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward +  gamma * np.max(targetNet.forward(observation))\n",
    "        error = [-1 * (target - x[action]) if action == i else 0 for i in [0,1]]\n",
    "        net.backward(np.array(error).reshape(-1))\n",
    "        net.update(learning_rate)\n",
    "    if epsilon > 0.11 :\n",
    "        epsilon *= epsilonDecay\n",
    "    counter += 1\n",
    "    episode += 1\n",
    "    if counter == 1000:\n",
    "        print(episode, estimatePerformance())\n",
    "        counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 99.88937316, 102.72937192])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimatePerformance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [144], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m         error \u001b[38;5;241m=\u001b[39m  (reward \u001b[38;5;241m-\u001b[39m x[action]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m         error \u001b[38;5;241m=\u001b[39m [error \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m i \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m---> 26\u001b[0m         \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         net\u001b[38;5;241m.\u001b[39mupdate(learning_rate)\n\u001b[0;32m     29\u001b[0m targetNet \u001b[38;5;241m=\u001b[39m getTargetNet(net)\n",
      "Cell \u001b[1;32mIn [37], line 36\u001b[0m, in \u001b[0;36mNetwork.backward\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     35\u001b[0m     error \u001b[38;5;241m=\u001b[39m (error \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT)  \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradb[i] \u001b[38;5;241m=\u001b[39m error\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW \u001b[38;5;241m=\u001b[39m [ np\u001b[38;5;241m.\u001b[39mclip(w,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradW]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replayBuffer = []\n",
    "net = Network(4, 16, 2, 1)\n",
    "targetNet = getTargetNet(net)\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    for i in range(1000):\n",
    "        x = net.forward(state)\n",
    "        action = policy(x)\n",
    "\n",
    "        replayBuffer.append((state,x, action, env.step(action)))\n",
    "        observation, reward, terminated, info = replayBuffer[-1][-1]\n",
    "        if terminated:\n",
    "            env.reset()\n",
    "        if(len(replayBuffer) < 32):\n",
    "            continue\n",
    "        for frame in np.random.choice(range(len(replayBuffer)), 32, replace=False) :\n",
    "            frame = replayBuffer[frame]\n",
    "            state,x , action, step = frame\n",
    "            observation, reward, terminated, info = step\n",
    "            x = net.forward(state)\n",
    "            if not terminated: \n",
    "                reward += gamma * np.max(targetNet.forward(observation))\n",
    "            error =  (reward - x[action]) * -1\n",
    "            error = [error if action == i else 0 for i in [0,1]]\n",
    "            \n",
    "            net.backward(np.array(error))\n",
    "            net.update(learning_rate)\n",
    "    \n",
    "    targetNet = getTargetNet(net)\n",
    "    if epsilon > 10 ** -1:\n",
    "        epsilon *= epsilonDecay\n",
    "    replayBuffer = []\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a59fb673d176b02a6017ed87a6c53d78aed30df45046877791ad08793a0c669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
