{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'terminated'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        for i in range(num_layers -1): \n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim ))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        return self.layers(torch.tensor(x, device=device, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 128\n",
    "gamma = torch.tensor(0.99)\n",
    "gamma.to(device)\n",
    "epsilon = 1\n",
    "EPS_END = 0.10\n",
    "EPS_DECAY = 0.99\n",
    "TARGET_UPDATE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(4, 128, 2, 2).to(device)\n",
    "target_net = DQN(4, 128, 2, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "memory = ReplayMemory(100000)\n",
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(x, epsilon):\n",
    "    if torch.rand(1) < epsilon:\n",
    "        return torch.tensor(random.choice([0,1]))\n",
    "    else:\n",
    "        return x.max(0)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0371, -0.0433], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "policy_net(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimatePerformance():  \n",
    "    counters = []\n",
    "    epsilon = -1\n",
    "    trials = 10\n",
    "    for i in range(trials):\n",
    "        counter = 0\n",
    "        terminated = False\n",
    "        observation = env.reset()\n",
    "        while not terminated:\n",
    "            action = policy_net(observation).max(0)[-1].item()\n",
    "            observation, reward, terminated,  info = env.step(action)\n",
    "            counter += reward\n",
    "        counters.append(counter)\n",
    "\n",
    "    summ =  0\n",
    "    for i in range(trials):\n",
    "        summ += counters[i]\n",
    "    return summ/trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def updateTargetNet():\n",
    "    target_net = deepcopy(policy_net)\n",
    "    for param in target_net.parameters():\n",
    "        param.requires_grad = False\n",
    "    return target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "epi_count = 0\n",
    "cum_reward = 0.0\n",
    "upd_count = 0\n",
    "loss = \"untrained\"\n",
    "terminated = False\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode=23602, reward=482.8, loss=1.6255673170089722, epsilon=0.3024044356690215 iter=23549, example _ prediction = 128.5148468017578\n",
      "Episode=23702, reward=497.1, loss=1.185368537902832, epsilon=0.3024044356690215 iter=23649, example _ prediction = 137.26695251464844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     x \u001b[39m=\u001b[39m policy_net(observation)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m action \u001b[39m=\u001b[39m policy(x, epsilon)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m next_state, reward, terminated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action\u001b[39m.\u001b[39mitem())\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb Cell 13\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(x, epsilon)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpolicy\u001b[39m(x, epsilon):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m) \u001b[39m<\u001b[39;49m epsilon:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(random\u001b[39m.\u001b[39mchoice([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/k/dev/RL/DQL/DQL_torch_batch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if counter % TARGET_UPDATE == 0:\n",
    "        target_net = updateTargetNet()\n",
    "    while not terminated:\n",
    "        with torch.no_grad():\n",
    "            x = policy_net(observation)\n",
    "        action = policy(x, epsilon)\n",
    "        next_state, reward, terminated, _ = env.step(action.item())\n",
    "        reward = 1\n",
    "        cum_reward += reward\n",
    "        memory.push(observation, action, next_state, reward, terminated)\n",
    "        observation = (next_state)\n",
    "    observation = env.reset()\n",
    "    terminated = False\n",
    "    epi_count += 1\n",
    "    if len(memory) >= 10*batchSize:\n",
    "        transitions = memory.sample(batchSize)\n",
    "        tup = np.stack([transition for transition in transitions])\n",
    "        obs = np.stack(tup[:, 0])\n",
    "        action = np.stack(tup[:, 1])\n",
    "        obs_next = np.stack(tup[:, 2])\n",
    "        rew = torch.tensor(np.stack(tup[:, 3]), dtype=torch.float)\n",
    "        term = torch.tensor(np.stack(tup[:, 4]), dtype=torch.float)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_prime = target_net(obs_next).max(dim=1)[0]\n",
    "        target =rew + gamma * q_prime * (1 - term)\n",
    "\n",
    "        q_pred = policy_net(obs)[np.arange(obs.shape[0]), action]\n",
    "        loss = mse(q_pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        upd_count += 1\n",
    "    if len(memory) >=batchSize and (counter % 200 == 0):\n",
    "        epsilon = max(EPS_END, epsilon*EPS_DECAY)\n",
    "    if counter %100 == 0:\n",
    "        print(f\"Episode={epi_count}, reward={estimatePerformance()}, loss={loss}, epsilon={epsilon} iter={upd_count}, example _ prediction = {q_pred[0]}\")\n",
    "    counter += 1\n",
    "    cum_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batu/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arrays = [asanyarray(arr) for arr in arrays]\n"
     ]
    }
   ],
   "source": [
    "transitions = memory.sample(30)\n",
    "tup = np.stack([transition for transition in transitions])\n",
    "obs = np.stack(tup[:, 0])\n",
    "action = np.stack(tup[:, 1])\n",
    "obs_next = np.stack(tup[:, 2])\n",
    "rew = torch.tensor(np.stack(tup[:, 3]), dtype=torch.float)\n",
    "term = torch.tensor(np.stack(tup[:, 4]), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1189, 1.0976, 1.0937, 1.0713, 1.1059, 1.1081, 1.1093, 1.1100, 1.1089,\n",
       "        1.0933, 1.1081, 1.0922, 1.1146, 1.1010, 1.1109, 1.0913, 1.1187, 1.0939,\n",
       "        0.9757, 1.1005, 1.1076, 1.1043, 1.0712, 1.1040, 1.1058, 1.1105, 1.1048,\n",
       "        1.0744, 1.0857, 1.1042, 1.1158, 1.1101, 1.0931, 1.1075, 1.0972, 1.1184,\n",
       "        1.0836, 1.1012, 1.0906, 1.0899, 1.1055, 1.1034, 1.1147, 1.0958, 1.1247,\n",
       "        1.1118, 1.1213, 1.1173, 1.1009, 1.1073, 1.0944, 1.1074, 1.0753, 1.0902,\n",
       "        1.1072, 1.1060, 1.1200, 1.0952, 1.1061, 1.0688, 1.1205, 1.1081, 1.1003,\n",
       "        1.0912, 1.1035, 1.1201, 1.1243, 1.1024, 1.0882, 1.0935, 1.1133, 1.0798,\n",
       "        1.1008, 1.1278, 1.0944, 1.1035, 1.1089, 1.1053, 1.1141, 1.0422, 1.0914,\n",
       "        1.1218, 1.0951, 1.1047, 1.0939, 1.0955, 1.1088, 1.1183, 1.0966, 1.1084,\n",
       "        1.0968, 1.1060, 1.0891, 1.0951, 1.0887, 1.0677, 1.1053, 1.1069, 1.1306,\n",
       "        1.1346, 1.0942, 1.1171, 1.1053, 1.0948, 1.1015, 1.0895, 1.1037, 1.0915,\n",
       "        1.1215, 1.0675, 1.0847, 1.1172, 1.0862, 1.0871, 1.1131, 1.1101, 1.1159,\n",
       "        1.0877, 1.0888, 1.1031, 1.0930, 1.1047, 1.1207, 1.1070, 1.0883, 1.0884,\n",
       "        1.0461, 1.1022], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(obs)[np.arange(obs.shape[0]), action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0935, 1.1059], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " policy_net(obs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action= np.stack(tup[:, 1])[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
